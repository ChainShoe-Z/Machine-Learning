# -*- coding: utf-8 -*-
"""a2_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k2V51D3E7jIUJz1rmmIFaxEn5K0j_C7w
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.svm import SVC
from sklearn.datasets import make_circles,make_moons,make_blobs,make_classification
from time import time
import datetime

train_df = pd.read_csv(r'fashion-mnist_train.csv')
#test_df = pd.read_csv(r'fashion-mnist_test.csv')
#train_df.head()

my_data = train_df[59000:]


my_data

my_data_arr = np.array(my_data, dtype='float32')
#test_data = np.array(test_df, dtype='float32')

x_data = my_data_arr[:,1:] /255
y_data = my_data_arr[:,0]



v=[[1,2,4,2,2,4,5,6,],[2,35,5,5,5,5,3,6],[4,5,2,2,4,6,7,6],[4,5,6,4,34,5,3,56]]
k=np.array(v)
k
my_data_arr
k
x_da=k[1:,1:]
x_da

my_data_arr
x_data

def getscore(clf, x_train, x_test, y_train, y_test):
  clf.fit(x_train, y_train)
  return clf.score(x_test, y_test)

from sklearn.model_selection import StratifiedKFold
folds = StratifiedKFold(n_splits=10)

from sklearn.model_selection import KFold
kf = KFold(n_splits=5)
kf

from sklearn.linear_model import LogisticRegression
from numpy import *
from sklearn import svm
from sklearn.svm import SVC
import matplotlib.pyplot as plt

score1=[]
score2=[]

score1_c=[]
score2_c=[]

for train_index,test_index in kf.split(x_data):
  x_train,x_test,y_train,y_test = x_data[train_index],x_data[test_index], y_data[train_index], y_data[test_index]
  #score1.append(getscore(LogisticRegression(C=0.9,penalty='l2',solver='lbfgs',multi_class='multinomial'),x_train,x_test,y_train,y_test))
  #score2.append(getscore(SVC(kernel='linear',C=20),x_train,x_test,y_train,y_test))

  c1_try = np.linspace(0.01,1,10)
  c2_try = np.linspace(0.01,30,30)

  c3_try = np.linspace(0.01,1,10)
  
  c_value1=[]
  c_value2=[]

  c_value3=[]

  score1_each=[]
  score2_each=[]

  score3_each=[]

  best_c1=[]
  best_c2=[]

  best_c3=[]

  for r in c3_try:
    score3_each.append(getscore(SVC(kernel='rbf', C=1.04, gamma=r), x_train, x_test, y_train, y_test)) #score1_each: score of each c value
    c_value3.append(r) #each c value
  plt.figure()
  plt.plot(score3_each, c_value3)
  #score2.append(max(score2_each)) #score1: max scores
  #best_c2.append(c_value2[score2_each.index(max(score2_each))])

from sklearn.linear_model import LogisticRegression
from numpy import *
from sklearn import svm
from sklearn.svm import SVC
import matplotlib.pyplot as plt

score1=[]
score2=[]

score1_c=[]
score2_c=[]

for train_index,test_index in kf.split(x_data):
  x_train,x_test,y_train,y_test = x_data[train_index],x_data[test_index], y_data[train_index], y_data[test_index]
  #score1.append(getscore(LogisticRegression(C=0.9,penalty='l2',solver='lbfgs',multi_class='multinomial'),x_train,x_test,y_train,y_test))
  #score2.append(getscore(SVC(kernel='linear',C=20),x_train,x_test,y_train,y_test))

  c1_try = np.linspace(0.01,1,10)
  c2_try = np.linspace(0.01,30,30)

  c3_try = np.linspace(0.0001,1,10)
  
  c_value1=[]
  c_value2=[]

  c_value3=[]

  score1_each=[]
  score2_each=[]

  score3_each=[]

  best_c1=[]
  best_c2=[]

  best_c3=[]

  for i in c1_try:
    score1_each.append(getscore(LogisticRegression(C=i,penalty='l2',solver='lbfgs',multi_class='multinomial'),x_train,x_test,y_train,y_test)) #score1_each: score of each c value
    c_value1.append(i) #each c value
  score1.append(max(score1_each)) #score1: max scores
  best_c1.append(c_value1[score1_each.index(max(score1_each))])

  for t in c2_try:
    score2_each.append(getscore(SVC(kernel='linear', C=t), x_train, x_test, y_train, y_test)) #score1_each: score of each c value
    c_value2.append(t) #each c value
  score2.append(max(score2_each)) #score1: max scores
  best_c2.append(c_value2[score2_each.index(max(score2_each))])

  for r in c2_try:
    score3_each.append(getscore(SVC(kernel='rbf', C=1.04, gamma=r), x_train, x_test, y_train, y_test)) #score1_each: score of each c value
    c_value3.append(r) #each c value
  plt.figure()
  plt.plot(score3_each, c_value3)
  #score2.append(max(score2_each)) #score1: max scores
  #best_c2.append(c_value2[score2_each.index(max(score2_each))])


print("Highest score for each iteration in Logistic Regression:")
print(score1)
print("\n")
print("The best C value overall in Logistic Regression: ")
print(best_c1)


print("Highest score for each iteration in linear SVM:")
print(score2)
print("\n")
print("The best C value overall in linear SVM: ")
print(best_c2)

######################################################################







def kcross(dataset_ori,k,clf):
  each_size = len(x_ori)/k
  new_x_train=[]
  new_y_train=[]
  new_x_test=[]
  new_y_test=[]
  for i in range(k):
    new_train = dataset_ori[0:i+each_size]

x=np.array([[1,2],[3,4],[1,3],[3,5],[1,2],[3,6],[6,7],[5,5],[8,8],[8,2],[9,1],[0,3]])
y=np.array([1,2,3,4,5,6,7,8,9,10,11,12])

z=[]
#z.append(x[0:2,:])


z.append(x[0:3,:])
z

from sklearn.model_selection import KFold
kf = KFold(n_splits=10)

index=[]
for i in range(2000):
  index.append(i)


for train_index, test_index, in kf.split(index):
  print(train_index,test_index)

def my_split(size,k):
  each_size = size%k
  train_index=[]
  test_index=[]
  x=np.zeros((k,2))
  for i in range(k):
    for j in range(each_size):
      x[i,1](test_index.append(i+j))
  
  print(test_index)

my_split(100,3)