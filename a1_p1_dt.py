# -*- coding: utf-8 -*-
"""A1_p1_dt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CHAEk09KF8Swhy_yNcMtLK02udT_WR0Z
"""

#I put the data into a csv file, then upload and read the csv file
from google.colab import files
uploaded = files.upload()

import pandas as pd
#give titles to all features and class
data = pd.read_csv('testdata.csv', names=['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldspeak','slope','ca','thal','num'])

from sklearn.tree import DecisionTreeClassifier
import sklearn.datasets as datasets
import numpy as np

#show the table
data

#extrac data and print x and y
x = data.iloc[:,0:13]
y = data.iloc[:,13]
print(x)
print(y)

#-----------------------------------------Model 1--------------------------------------------------#
#split training data and test data, use test size as 0.2
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)

#give feature names to clf as param
fn=['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldspeak','slope','ca','thal']

#the most basic model, no prunning, after my tests, score is in [0.61,0.85], mostly around 0.7
clf = DecisionTreeClassifier()
clf.fit(x_train,y_train)
clf.score(x_test,y_test)

#score of the train set
score_train=clf.score(x_train,y_train)
score_train

#show importance of features in this model
clf.feature_importances_

#visualize Model 1
import graphviz
from sklearn import tree

thisdata = tree.export_graphviz(clf,out_file=None,filled=True,feature_names=fn)
graph = graphviz.Source(thisdata)
graph

#output the pdf file of this tree
graph.render('DT1')

##-----------------------------------------Model 1 END---------------------------------------------#

#-----------------------------------------Model 2--------------------------------------------------#
#split training data and test data, use test size as 0.2
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)

#give feature names to clf as param
fn=['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldspeak','slope','ca','thal']

#used to find the best max_depth
import matplotlib.pyplot as plt
from matplotlib.pyplot import*
test=[]
for i in range(10):
  clf = tree.DecisionTreeClassifier(max_depth=i+1
                    ,random_state=30
                                    )
  clf = clf.fit(x_train,y_train)
  score = clf.score(x_test, y_test)
  test.append(score)
plt.plot(range(1,11),test,color="red",label="max_depth")
plt.legend()
plt.show()

#final model with parameters
clf = DecisionTreeClassifier(random_state=30
                             , max_depth=6
                             , min_samples_leaf=14
                             , min_samples_split=19
                             , max_features=8
                             )
clf.fit(x_train,y_train)
clf.score(x_test,y_test)

#visualize the Model 2
thisdata = tree.export_graphviz(clf,out_file=None,filled=True,feature_names=fn)
graph = graphviz.Source(thisdata)
graph

#show importaances of each features
clf.feature_importances_

#the training score, a value less than 1 is good,  since it means the overfit is been reduced 
score_train=clf.score(x_train,y_train)
score_train
#----------------------------Model 2 END-------------------------------------------------#